reward model 

takes in a sequence of text -> returns a scalar. 
This is crucial in the RLHF process. 
1. initial language model that generates text
2. model that takes in any text and assignt it a score 

Policy 
IN: promopt
OUT: sequence of text. (within vocabulary)

Action space: 
All the tokens corresponding to the vocabulary of the lanuga model 

Observation space: 
distribution of possible input token sequences. 

reward function: 
combination of the preference model and a constraint on policy shift 